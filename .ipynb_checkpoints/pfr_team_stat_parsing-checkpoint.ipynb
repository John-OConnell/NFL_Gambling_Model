{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e06db600",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (vardata.py, line 21)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3369\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  Input \u001b[0;32mIn [1]\u001b[0;36m in \u001b[0;35m<cell line: 7>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from vardata import years, team_abbr, columns, teams\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/Documents/Projects/winners/NFL_Gambling_Model/vardata.py:21\u001b[0;36m\u001b[0m\n\u001b[0;31m    'Las Vegas Raiders':'rai','Oakland Raiders':'rai,\u001b[0m\n\u001b[0m                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "from vardata import years, team_abbr, columns, teams\n",
    "\n",
    "url = 'https://www.pro-football-reference.com'\n",
    "\n",
    "# function to convert output a dataframe as a csv\n",
    "def df_to_csv(df, year):\n",
    "    exec(\"df.to_csv('stats_\" + year + \".csv')\")\n",
    "\n",
    "    \n",
    "def scrape_stats(team, year):\n",
    "    \n",
    "    print(team)\n",
    "    # use beautiful soup to return the html of the webpage\n",
    "    r = requests.get(url + '/teams/' + team + '/' + year + '.htm')\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    \n",
    "    # grab the team stats table from the team webpage\n",
    "    team_stats = soup.find_all('table')[0]\n",
    "    # grab the team conversions table from the team webpage\n",
    "    team_conv = soup.find_all('table')[2]\n",
    "\n",
    "    # Collect table rows from stats table\n",
    "    stat_rows = team_stats.findAll('tr')[2:4]\n",
    "    # Get stats from each row\n",
    "    stats = []\n",
    "    for i in range(len(stat_rows)):\n",
    "        stats.extend([float(col.getText()) for col in stat_rows[i].findAll('td')[:25]])\n",
    "\n",
    "    # Collect table rows from conversions table\n",
    "    conv_rows = team_conv.findAll('tr')[2:4]\n",
    "    # Get stats from each row\n",
    "    convs = []\n",
    "    for i in range(len(conv_rows)):\n",
    "        convs.extend([float(col.getText().strip('%')) for col in conv_rows[i].findAll('td')])\n",
    "   \n",
    "    # combine all stats\n",
    "    stats.extend(convs)\n",
    "    # insert the team abbrev at beggining of array\n",
    "    stats.insert(0, team)\n",
    "    print(stats)\n",
    "    return stats\n",
    "    \n",
    "# ___________________________________\n",
    "\n",
    "#intialize num to act as counter and prevent over scrapping\n",
    "num = 1\n",
    "\n",
    "# loop through all years in vardata file\n",
    "for year in years:\n",
    "    all_stats = []\n",
    "    print(year)\n",
    "    # loop through each team listed in team abbrev list\n",
    "    for team in team_abbr:\n",
    "        # check to prevent overloading webpage with requests\n",
    "        if num < 20:\n",
    "            # grab stats from specific team and year\n",
    "            stats = scrape_stats(team, year)\n",
    "            # append to global stats array\n",
    "            all_stats.append(stats)\n",
    "            num += 1\n",
    "        else:\n",
    "            # grab stats from specific team and year\n",
    "            stats = scrape_stats(team, year)\n",
    "            # append to global stats array\n",
    "            all_stats.append(stats)\n",
    "            time.sleep(65)\n",
    "            num = 1\n",
    "    # convert global stats array to pandas dataframe\n",
    "    exec('ts_' + year + ' = pd.DataFrame(all_stats, columns = columns)')\n",
    "    # save dataframe to csv\n",
    "    exec('df_to_csv(ts_'+ year + ', year)')\n",
    "    # pause a random amount of time (from 1-5 mins) to prevent\n",
    "    # overloading webpage with requests\n",
    "    wait = random.randint(65, 300)\n",
    "    time.sleep(wait)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
