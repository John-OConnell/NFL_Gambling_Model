{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5643938",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Comment\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from vardata import years, team_abbr, columns, teams, teams_wkly, weekly_columns\n",
    "\n",
    "year = '2023'\n",
    "week = '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbecf87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.pro-football-reference.com'\n",
    "\n",
    "# function to convert output a dataframe as a csv\n",
    "def df_to_csv(df, year, week):\n",
    "    exec(\"df.to_csv('games_\" + year + \"_week_\" + week + \".csv')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d1b5c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_games(year, week):\n",
    "    \n",
    "    # use beautiful soup to return the html of the webpage\n",
    "    r = requests.get(url + '/years/'+ year + '/games.htm')\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    \n",
    "    # grab the game results table from the webpage\n",
    "    game_stats = soup.find_all('table')[0]\n",
    "\n",
    "    # collect table rows\n",
    "    stat_rows = game_stats.findAll('tr')\n",
    "    \n",
    "    # initalize \n",
    "    results = []\n",
    "    n = 0\n",
    "    \n",
    "    # looping through the table rows to collect data\n",
    "    for i in range(len(stat_rows)):\n",
    "        if n >= 19:\n",
    "            wait = random.randint(65, 120)\n",
    "            time.sleep(wait)\n",
    "            n = 0\n",
    "        \n",
    "        if stat_rows[i].find('th').getText() != week:\n",
    "            continue\n",
    "            \n",
    "        row = []\n",
    "        row.extend([col.getText() for col in stat_rows[i].findAll('td')])\n",
    "\n",
    "#         if not row:\n",
    "#             continue\n",
    "#         elif row[1] == 'Playoffs':\n",
    "#             break\n",
    "        \n",
    "        # navigate to the preview page and find the vegas line\n",
    "        #-------------------------------------------------------------------------\n",
    "        # get box score url\n",
    "        link = stat_rows[i].find(href=re.compile(\"boxscores\"))\n",
    "        href=link.get('href')\n",
    "        print(row)\n",
    "        \n",
    "        # use beautiful soup to return the html of the webpage\n",
    "        r1 = requests.get(url + href)\n",
    "        soup1 = BeautifulSoup(r1.content, 'html.parser')\n",
    "        \n",
    "        \n",
    "        mydivs = soup1.find(\"div\", class_=\"scorebox_meta\")\n",
    "        \n",
    "        vegas_line = mydivs.find_next(\"div\")\n",
    "\n",
    "        while (vegas_line.text.split())[0] != \"Vegas:\":\n",
    "            vegas_line = vegas_line.find_next(\"div\")\n",
    "        vegas_line = vegas_line.text\n",
    "        \n",
    "        favorite = ' '.join(vegas_line.split()[1:-1])\n",
    "        favorite = teams_wkly[favorite]\n",
    "        vegas_line = vegas_line.split()[-1]\n",
    "        \n",
    "        if vegas_line == 'Pick':\n",
    "            vegas_line = 0\n",
    "        else:\n",
    "            vegas_line = float(vegas_line)\n",
    "        #-------------------------------------------------------------------------\n",
    "        \n",
    "        # handling home / away team info being in different locations for each row\n",
    "        if row[4] != '@':\n",
    "            home_team = teams[row[3]]\n",
    "            away_team = teams[row[5]]\n",
    "\n",
    "            if row[3] == favorite:\n",
    "                ht_vegas_line = vegas_line\n",
    "            else:\n",
    "                ht_vegas_line = abs(vegas_line)\n",
    "        else:\n",
    "            home_team = teams[row[5]]\n",
    "            away_team = teams[row[3]]\n",
    "\n",
    "            if row[3] == favorite:\n",
    "                ht_vegas_line = abs(vegas_line)\n",
    "            else:\n",
    "                ht_vegas_line = vegas_line\n",
    "\n",
    "        # appending necessary info to results array\n",
    "        results.append([home_team, away_team, ht_vegas_line])\n",
    "\n",
    "        n += 1\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fafd95e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thu', '2023-10-05', '8:15PM', 'Chicago Bears', '@', 'Washington Commanders', 'preview', '', '', '', '', '', '']\n",
      "['Sun', '2023-10-08', '9:30AM', 'Jacksonville Jaguars', '@', 'Buffalo Bills', 'preview', '', '', '', '', '', '']\n",
      "['Sun', '2023-10-08', '1:00PM', 'Carolina Panthers', '@', 'Detroit Lions', 'preview', '', '', '', '', '', '']\n",
      "['Sun', '2023-10-08', '1:00PM', 'Houston Texans', '@', 'Atlanta Falcons', 'preview', '', '', '', '', '', '']\n",
      "['Sun', '2023-10-08', '1:00PM', 'New Orleans Saints', '@', 'New England Patriots', 'preview', '', '', '', '', '', '']\n",
      "['Sun', '2023-10-08', '1:00PM', 'New York Giants', '@', 'Miami Dolphins', 'preview', '', '', '', '', '', '']\n",
      "['Sun', '2023-10-08', '1:00PM', 'Tennessee Titans', '@', 'Indianapolis Colts', 'preview', '', '', '', '', '', '']\n",
      "['Sun', '2023-10-08', '1:00PM', 'Baltimore Ravens', '@', 'Pittsburgh Steelers', 'preview', '', '', '', '', '', '']\n",
      "['Sun', '2023-10-08', '4:05PM', 'Cincinnati Bengals', '@', 'Arizona Cardinals', 'preview', '', '', '', '', '', '']\n",
      "['Sun', '2023-10-08', '4:05PM', 'Philadelphia Eagles', '@', 'Los Angeles Rams', 'preview', '', '', '', '', '', '']\n",
      "['Sun', '2023-10-08', '4:25PM', 'Kansas City Chiefs', '@', 'Minnesota Vikings', 'preview', '', '', '', '', '', '']\n",
      "['Sun', '2023-10-08', '4:25PM', 'New York Jets', '@', 'Denver Broncos', 'preview', '', '', '', '', '', '']\n",
      "['Sun', '2023-10-08', '8:20PM', 'Dallas Cowboys', '@', 'San Francisco 49ers', 'preview', '', '', '', '', '', '']\n",
      "['Mon', '2023-10-09', '8:15PM', 'Green Bay Packers', '@', 'Las Vegas Raiders', 'preview', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "results = scrape_games(year, week)\n",
    "exec('games_' + year + '_week_' + week + '''= pd.DataFrame(results, columns = ['home_team', 'away_team', \n",
    "            'ht_vegas_line'])''')\n",
    "exec('df_to_csv(games_'+ year + '_week_' + week +', year, week)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baea4e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_stats(team, year):\n",
    "    \n",
    "    print(team)\n",
    "    # use beautiful soup to return the html of the webpage\n",
    "    r = requests.get(url + '/teams/' + team + '/' + year + '.htm')\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    \n",
    "    # grab the team stats table from the team webpage\n",
    "    team_stats = soup.find_all('table')[0]\n",
    "    # grab the team conversions table from the team webpage\n",
    "    team_conv = soup.find_all('table')[2]\n",
    "\n",
    "    # Collect table rows from stats table\n",
    "    stat_rows = team_stats.findAll('tr')[2:4]\n",
    "    # Get stats from each row\n",
    "    stats = []\n",
    "    for i in range(len(stat_rows)):\n",
    "        stats.extend([float(col.getText() or 0) for col in stat_rows[i].findAll('td')[:25]])\n",
    "\n",
    "    # Collect table rows from conversions table\n",
    "    conv_rows = team_conv.findAll('tr')[2:4]\n",
    "    # Get stats from each row\n",
    "    convs = []\n",
    "    for i in range(len(conv_rows)):\n",
    "        convs.extend([float(col.getText().strip('%') or 0) for col in conv_rows[i].findAll('td')])\n",
    "   \n",
    "    # combine all stats\n",
    "    stats.extend(convs)\n",
    "    # insert the team abbrev at beggining of array\n",
    "    stats.insert(0, team)\n",
    "    # print(stats)\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5adc0355",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "was\n",
      "chi\n",
      "buf\n",
      "jax\n",
      "det\n",
      "car\n",
      "atl\n",
      "htx\n",
      "nwe\n",
      "nor\n",
      "mia\n",
      "nyg\n",
      "clt\n",
      "oti\n",
      "pit\n",
      "rav\n",
      "crd\n",
      "cin\n",
      "ram\n",
      "phi\n",
      "min\n",
      "kan\n",
      "den\n",
      "nyj\n",
      "sfo\n",
      "dal\n",
      "rai\n",
      "gnb\n"
     ]
    }
   ],
   "source": [
    "weekly_data = pd.DataFrame(columns = weekly_columns)\n",
    "\n",
    "games = pd.read_csv('games_'+ year + '_week_' + week + '.csv')\n",
    "\n",
    "n = 1\n",
    "\n",
    "# loop through every game\n",
    "for index in games.index:\n",
    "    \n",
    "    if n >= 26:\n",
    "        time.sleep(65)\n",
    "        n = 1\n",
    "\n",
    "    # get home and away team indicies\n",
    "    home_team = games['home_team'][index]\n",
    "    away_team = games['away_team'][index]\n",
    "    vegas_line = games['ht_vegas_line'][index]\n",
    "    \n",
    "    home_stats = scrape_stats(home_team, year)\n",
    "    away_stats = scrape_stats(away_team, year)\n",
    "    \n",
    "    temp = [home_team, away_team]\n",
    "    \n",
    "    # append average of home stats with respect to away team stats\n",
    "    for i in range(25):\n",
    "        temp.append(np.mean((home_stats[i+1],away_stats[26+i])))\n",
    "    for i in range(25):\n",
    "        temp.append(np.mean((away_stats[i+1], home_stats[26+i])))\n",
    "    for i in range(9):\n",
    "        temp.append(np.mean((home_stats[51+i],away_stats[60+i])))\n",
    "    for i in range(9):\n",
    "        temp.append(np.mean((away_stats[51+i],home_stats[60+i])))\n",
    "    \n",
    "    weekly_data.loc[len(weekly_data)] = temp\n",
    "    \n",
    "    n+=2\n",
    "    \n",
    "weekly_data.to_csv('stats_' + year + '_week_' + week + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cebbcc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
